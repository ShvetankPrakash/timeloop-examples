{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9cbbcb1-1d30-4f63-8caf-c9608e7c9082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaldiio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e3c603-30fa-460a-9b0c-9a40aea4c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FHSData(torch.utils.data.Dataset):\n",
    "    def __init__(self, feats_scp, utt2label, cmvn_file=None):\n",
    "        super().__init__()\n",
    "        self.uttids = []\n",
    "        self.utt2feats = {}\n",
    "        self.utt2labs = {}\n",
    "        with open(feats_scp) as f:\n",
    "            for line in f:\n",
    "                splits = line.rstrip().split()\n",
    "                self.uttids.append(splits[0])\n",
    "                self.utt2feats[splits[0]] = splits[1]\n",
    "        \n",
    "        with open(utt2label) as f:\n",
    "            for line in f:\n",
    "                splits = line.rstrip().split()\n",
    "                self.utt2labs[splits[0]] = int(splits[1])\n",
    "        \n",
    "        if cmvn_file is not None:\n",
    "            self.cmvn = self._load_cmvn(cmvn_file)\n",
    "        else:\n",
    "            self.cmvn = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.uttids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        uttid = self.uttids[idx]\n",
    "        feats_path = self.utt2feats[uttid]\n",
    "        features = kaldiio.load_mat(feats_path).copy()\n",
    "        if self.cmvn is not None:\n",
    "            features = self._apply_cmvn(features, self.cmvn)\n",
    "        label = self.utt2labs[uttid]\n",
    "        return {'uttid': uttid, 'feats': torch.tensor(features).float(),\n",
    "               'target': torch.tensor(label).long()}\n",
    "    \n",
    "    def _load_cmvn(self, cmvn_file):\n",
    "        cmvn = kaldiio.load_mat(cmvn_file)\n",
    "        assert cmvn.shape[0] == 2\n",
    "        cnt = cmvn[0, -1]\n",
    "        sums = cmvn[0, :-1]\n",
    "        sums2 = cmvn[1, :-1]\n",
    "        means = sums / cnt\n",
    "        stds = np.sqrt(np.maximum(1e-10, sums2 / cnt - means ** 2))\n",
    "        return means, stds\n",
    "    \n",
    "    def _apply_cmvn(self, features, cmvn):\n",
    "        # https://github.com/kaldi-asr/kaldi/blob/master/src/transform/cmvn.cc\n",
    "        means, stds = cmvn\n",
    "        features -= means\n",
    "        features /= stds\n",
    "        return features\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    # max_len = max(len(ex['feats']) for ex in batch)\n",
    "    max_len = 1024\n",
    "    batch_feats = torch.zeros(len(batch), max_len, batch[0]['feats'].shape[-1])\n",
    "    batch_targets = []\n",
    "    batch_lens = []\n",
    "    for i, ex in enumerate(batch):\n",
    "        feats = ex['feats']\n",
    "        tgt = ex['target']\n",
    "        batch_feats[i, :len(feats)] = feats\n",
    "        batch_lens.append(len(feats))\n",
    "        batch_targets.append(tgt)\n",
    "    \n",
    "    batch_targets = torch.stack(batch_targets, dim=0).long()\n",
    "    batch_lens = torch.tensor(batch_lens).long()\n",
    "    return {'feats': batch_feats,'feats_len': batch_lens, 'targets': batch_targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b23557-0881-48e4-b1e3-9310b71b4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_wav_scp = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/train_fra/feats.scp'\n",
    "tr_utt2label = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/train_fra/utt2label'\n",
    "cmvn_file = '/data/sls/scratch/sameerk/fhs_prepared/kaldi_data/fbanks/cmvn.ark'\n",
    "tr_ds = FHSData(tr_wav_scp, tr_utt2label, cmvn_file)\n",
    "tr_ds = torch.utils.data.DataLoader(tr_ds, batch_size=32, collate_fn=_collate_fn, drop_last=False, \n",
    "                                    num_workers=2, shuffle=True)\n",
    "\n",
    "dev_wav_scp = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/dev_fra/feats.scp'\n",
    "dev_utt2label = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/dev_fra/utt2label'\n",
    "cmvn_file = '/data/sls/scratch/sameerk/fhs_prepared/kaldi_data/fbanks/cmvn.ark'\n",
    "dev_ds = FHSData(dev_wav_scp, dev_utt2label, cmvn_file)\n",
    "dev_ds = torch.utils.data.DataLoader(dev_ds, batch_size=32, collate_fn=_collate_fn, drop_last=False, \n",
    "                                    num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aca3da-c282-4d50-af61-47d708624949",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in tr_ds:\n",
    "    print(ex['feats'].shape)\n",
    "    print(ex['targets'])\n",
    "    print(ex['feats_len'])\n",
    "    break\n",
    "\n",
    "for ex in dev_ds:\n",
    "    print(ex['feats'].shape)\n",
    "    print(ex['targets'])\n",
    "    print(ex['feats_len'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cadd5bd-95a9-4ca4-9b81-607e8c96927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # defining batchnorm input                                                                                               \n",
    "        # self.batchnorm1 = nn.BatchNorm2d(1)\n",
    "\n",
    "        # defining Convolutional layers                                                                                           \n",
    "        conv1_C = 256\n",
    "        conv2_C = 512\n",
    "        conv3_C = 1024\n",
    "        conv4_C = 2048\n",
    "        # conv5_C = hparams['out_channels'][4]\n",
    "        conv_W = 18\n",
    "        pad = int(np.floor(conv_W / 2) - 1)\n",
    "        stride_W = 2\n",
    "        pool_W = 3\n",
    "        self.conv1 = nn.Conv2d(1, conv1_C, kernel_size=(40,1), stride=(1,1), padding=(0,0))\n",
    "        self.bn1 = nn.BatchNorm2d(conv1_C)\n",
    "        self.conv2 = nn.Conv2d(conv1_C, conv2_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn2 = nn.BatchNorm2d(conv2_C)\n",
    "        self.conv3 = nn.Conv2d(conv2_C, conv3_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn3 = nn.BatchNorm2d(conv3_C)\n",
    "        self.conv4 = nn.Conv2d(conv3_C, conv4_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn4 = nn.BatchNorm2d(conv4_C)\n",
    "        # self.conv5 = nn.Conv2d(conv4_C, conv5_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "\n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "        \n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "\n",
    "        # Defining output layer                                                                                                   \n",
    "        input_W = 2 ** 11\n",
    "        num_layers = 3\n",
    "        conv_C = 2048\n",
    "        \n",
    "        if num_layers == 0:\n",
    "            num_layers = 1\n",
    "        # hard coded, figure it out manually\n",
    "        map_W = 255                                                                                  \n",
    "        embedding_dim = int(conv_C)\n",
    "        self.fc1 = nn.Linear(embedding_dim, num_classes)\n",
    "        self.fc2 = nn.Sigmoid()\n",
    "        \n",
    "        # Defining global average pooling                                                                                         \n",
    "        # self.poolMean = nn.AvgPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "\n",
    "        # Defining global max pooling                                                                                             \n",
    "        self.poolMax = nn.MaxPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        targets = batch['targets']\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # x = self.batchnorm1(x)\n",
    "        x = self.bn1(F.relu(self.conv1(x)))\n",
    "        x = self.bn2(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(F.relu(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.bn4(F.relu(self.conv4(x)))\n",
    "        x = self.poolMax(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "#### Model without FC layer #####\n",
    "class CNNNoFC(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # defining batchnorm input                                                                                               \n",
    "        # self.batchnorm1 = nn.BatchNorm2d(1)\n",
    "\n",
    "        # defining Convolutional layers                                                                                           \n",
    "        conv1_C = 256\n",
    "        conv2_C = 512\n",
    "        conv3_C = 1024\n",
    "        conv4_C = 2048\n",
    "        # conv5_C = hparams['out_channels'][4]\n",
    "        conv_W = 18\n",
    "        pad = int(np.floor(conv_W / 2) - 1)\n",
    "        stride_W = 2\n",
    "        pool_W = 3\n",
    "        self.conv1 = nn.Conv2d(1, conv1_C, kernel_size=(40,1), stride=(1,1), padding=(0,0))\n",
    "        self.bn1 = nn.BatchNorm2d(conv1_C)\n",
    "        self.conv2 = nn.Conv2d(conv1_C, conv2_C, kernel_size=(1, conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn2 = nn.BatchNorm2d(conv2_C)\n",
    "        self.conv3 = nn.Conv2d(conv2_C, conv3_C, kernel_size=(1, conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn3 = nn.BatchNorm2d(conv3_C)\n",
    "        self.conv4 = nn.Conv2d(conv3_C, conv4_C, kernel_size=(1, conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn4 = nn.BatchNorm2d(conv4_C)\n",
    "        # self.conv5 = nn.Conv2d(conv4_C, conv5_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "\n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "        \n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "\n",
    "        # Defining output layer                                                                                                   \n",
    "        input_W = 2 ** 11\n",
    "        num_layers = 3\n",
    "        conv_C = 2048\n",
    "        \n",
    "        if num_layers == 0:\n",
    "            num_layers = 1\n",
    "        # hard coded, figure it out manually\n",
    "        map_W = 255                                                                                  \n",
    "        embedding_dim = int(conv_C)\n",
    "        # self.fc1 = nn.Linear(embedding_dim, num_classes)\n",
    "        self.fc2 = nn.Sigmoid()\n",
    "        \n",
    "        # Defining global average pooling                                                                                         \n",
    "        # self.poolMean = nn.AvgPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "\n",
    "        # Defining global max pooling                                                                                             \n",
    "        self.poolMax = nn.MaxPool2d(kernel_size=(1, map_W), stride=(1,map_W), padding=(0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        targets = batch['targets']\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # x = self.batchnorm1(x)\n",
    "        x = self.bn1(F.relu(self.conv1(x)))\n",
    "        x = self.bn2(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(F.relu(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.bn4(F.relu(self.conv4(x)))\n",
    "        x = self.poolMax(x)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        # x = self.fc1(x)\n",
    "        # print(\"++\", x.shape)\n",
    "        x = torch.mean(x, dim=1)[:, None]\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5dd99-bb1b-44ba-9eb0-59d60641931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_org = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53252ba-a9c0-493d-8350-e1f2c1929e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e499f33-19c2-4ff8-9032-e0fe59a9c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(tr_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f302f6-633b-4135-b65c-14b26543d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cnn_org(batch['feats'])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0773f-fe5b-4da5-9e29-7b3be7f48296",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "for wd in [1e-7, 1e-5, 1e-3]:\n",
    "    for lr in [0.01, 0.005, 0.001, 0.0005]:\n",
    "        for seed in [1111, 2222, 1234, 3333]:\n",
    "            print(\"++++++ Exp seed %d wd %s lr %f ++++++++\" % (seed, str(wd), lr))\n",
    "            torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            model = CNN()\n",
    "            model.cuda()\n",
    "            optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "\n",
    "            auc_best = 0.\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for batch in tqdm.tqdm(tr_ds, total=len(tr_ds)):\n",
    "                    x = batch['feats']\n",
    "                    # use lens to do average pooling properly\n",
    "                    x_lens = batch['feats_len']\n",
    "                    tgts = batch['targets']\n",
    "                    x = x.cuda()\n",
    "                    tgts = tgts.cuda()\n",
    "                    x = model(x).squeeze(1)\n",
    "                    loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    epoch_loss += float(loss)\n",
    "                    y_pred.append(x.detach().cpu().numpy())\n",
    "                    y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                print('epoch loss: %f' % (epoch_loss/len(tr_ds)))\n",
    "                print('auc: %f' % auc)\n",
    "\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm.tqdm(dev_ds, total=len(dev_ds)):\n",
    "                        x = batch['feats']\n",
    "                        # use lens to do average pooling properly\n",
    "                        x_lens = batch['feats_len']\n",
    "                        tgts = batch['targets']\n",
    "                        x = x.cuda()\n",
    "                        tgts = tgts.cuda()\n",
    "                        x = model(x).squeeze(1)\n",
    "                        loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                        epoch_loss += float(loss)\n",
    "                        y_pred.append(x.detach().cpu().numpy())\n",
    "                        y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                if auc > auc_best:\n",
    "                    print(\"saving ckpt at auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    torch.save(model.state_dict(), \"auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    auc_best = auc\n",
    "\n",
    "                print('epoch dev loss: %f' % (epoch_loss/len(dev_ds)))\n",
    "                print('auc dev: %f' % auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfaa96-f155-46b8-9f30-771ae9626b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd24870-dff6-484e-aae4-7f8419a0010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred_class = (np.concatenate(y_pred)>0.5).astype(float)\n",
    "print(classification_report(np.concatenate(y_true), y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a57b0-d834-441b-9de1-5dad91a64f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNHalf(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(CNNHalf, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # defining batchnorm input                                                                                               \n",
    "        # self.batchnorm1 = nn.BatchNorm2d(1)\n",
    "\n",
    "        # defining Convolutional layers                                                                                           \n",
    "        conv1_C = 256\n",
    "        # conv2_C = 512\n",
    "        conv3_C = 512\n",
    "        # conv4_C = 2048\n",
    "        # conv5_C = hparams['out_channels'][4]\n",
    "        conv_W = 18\n",
    "        pad = int(np.floor(conv_W / 2) - 1)\n",
    "        stride_W = 2\n",
    "        pool_W = 3\n",
    "        self.conv1 = nn.Conv2d(1, conv1_C, kernel_size=(40,1), stride=(1,1), padding=(0,0))\n",
    "        self.bn1 = nn.BatchNorm2d(conv1_C)\n",
    "        # self.conv2 = nn.Conv2d(conv1_C, conv2_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        # self.bn2 = nn.BatchNorm2d(conv2_C)\n",
    "        self.conv3 = nn.Conv2d(conv1_C, conv3_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn3 = nn.BatchNorm2d(conv3_C)\n",
    "        # self.conv4 = nn.Conv2d(conv3_C, conv4_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        # self.bn4 = nn.BatchNorm2d(conv4_C)\n",
    "        # self.conv5 = nn.Conv2d(conv4_C, conv5_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "\n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "        \n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "\n",
    "        # Defining output layer                                                                                                   \n",
    "        input_W = 2 ** 11\n",
    "        num_layers = 3\n",
    "        conv_C = 512\n",
    "        \n",
    "        if num_layers == 0:\n",
    "            num_layers = 1\n",
    "        # hard coded, figure it out manually\n",
    "        map_W = 511                                                                                  \n",
    "        embedding_dim = int(conv_C)\n",
    "        self.fc1 = nn.Linear(embedding_dim, num_classes)\n",
    "        self.fc2 = nn.Sigmoid()\n",
    "        \n",
    "        # Defining global average pooling                                                                                         \n",
    "        # self.poolMean = nn.AvgPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "\n",
    "        # Defining global max pooling                                                                                             \n",
    "        self.poolMax = nn.MaxPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        targets = batch['targets']\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # x = self.batchnorm1(x)\n",
    "        x = self.bn1(F.relu(self.conv1(x)))\n",
    "        # x = self.bn2(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(F.relu(self.conv3(x)))\n",
    "        # x = self.pool(x)\n",
    "        # x = self.bn4(F.relu(self.conv4(x)))\n",
    "        x = self.poolMax(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c9a31-2e52-4832-b32f-fd1287dfb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_half = CNNHalf()\n",
    "cnn_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845f22f-dafb-4697-9d16-67ff9600f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_half = CNNHalf()\n",
    "batch = next(iter(tr_ds))\n",
    "x = cnn_half(batch['feats'])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c95dd-d83e-4470-af88-f87ca8e52651",
   "metadata": {},
   "source": [
    "### No KD small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a3556-fd4e-4a3e-bd5a-f02a346c2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "for wd in [1e-7]:\n",
    "    for lr in [0.01]:\n",
    "        for seed in [10001]:\n",
    "            print(\"++++++ Exp seed %d wd %s lr %f ++++++++\" % (seed, str(wd), lr))\n",
    "            torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            model = CNNHalf()\n",
    "            model.cuda()\n",
    "            optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "\n",
    "            auc_best = 0.\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for batch in tqdm.tqdm(tr_ds, total=len(tr_ds)):\n",
    "                    x = batch['feats']\n",
    "                    # use lens to do average pooling properly\n",
    "                    x_lens = batch['feats_len']\n",
    "                    tgts = batch['targets']\n",
    "                    x = x.cuda()\n",
    "                    tgts = tgts.cuda()\n",
    "                    x = model(x).squeeze(1)\n",
    "                    loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    epoch_loss += float(loss)\n",
    "                    y_pred.append(x.detach().cpu().numpy())\n",
    "                    y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                print('epoch loss: %f' % (epoch_loss/len(tr_ds)))\n",
    "                print('auc: %f' % auc)\n",
    "\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm.tqdm(dev_ds, total=len(dev_ds)):\n",
    "                        x = batch['feats']\n",
    "                        # use lens to do average pooling properly\n",
    "                        x_lens = batch['feats_len']\n",
    "                        tgts = batch['targets']\n",
    "                        x = x.cuda()\n",
    "                        tgts = tgts.cuda()\n",
    "                        x = model(x).squeeze(1)\n",
    "                        loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                        epoch_loss += float(loss)\n",
    "                        y_pred.append(x.detach().cpu().numpy())\n",
    "                        y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                if auc > auc_best:\n",
    "                    print(\"saving ckpt at auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    torch.save(model.state_dict(), \"auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    auc_best = auc\n",
    "\n",
    "                print('epoch dev loss: %f' % (epoch_loss/len(dev_ds)))\n",
    "                print('auc dev: %f' % auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa9d3c-1c98-4b04-a13c-1f18419658a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "model = CNNHalf()\n",
    "model.load_state_dict(torch.load('auc_0.874646_seed_3333_wd_1e-07_lr_0.010000.pt'))\n",
    "model.eval()\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(dev_ds, total=len(dev_ds)):\n",
    "        x = batch['feats']\n",
    "        # use lens to do average pooling properly\n",
    "        x_lens = batch['feats_len']\n",
    "        tgts = batch['targets']\n",
    "        x = x.cuda()\n",
    "        tgts = tgts.cuda()\n",
    "        x = model(x).squeeze(1)\n",
    "        y_pred.append(x.detach().cpu().numpy())\n",
    "        y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433d527-8bf9-439a-acf9-2faf44f83895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that in binary classification, recall of the positive class is also known as “sensitivity”; recall of the negative class is “specificity”.\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_class = (np.concatenate(y_pred)>0.5).astype(float)\n",
    "print(classification_report(np.concatenate(y_true), y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e006be0a-459d-4d9e-8db5-9b844a44b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class EffNetOri(nn.Module):\n",
    "    def __init__(self, label_dim=1, pretrain=False, model_id=0, audioset_pretrain=False):\n",
    "        super().__init__()\n",
    "        b = int(model_id)\n",
    "        print('now train a effnet-b{:d} model'.format(b))\n",
    "        if b == 7:\n",
    "            self.model = torchvision.models.efficientnet_b7(pretrained=pretrain)\n",
    "        elif b == 6:\n",
    "            self.model = torchvision.models.efficientnet_b6(pretrained=pretrain)\n",
    "        elif b == 5:\n",
    "            self.model = torchvision.models.efficientnet_b5(pretrained=pretrain)\n",
    "        elif b == 4:\n",
    "            self.model = torchvision.models.efficientnet_b4(pretrained=pretrain)\n",
    "        elif b == 3:\n",
    "            self.model = torchvision.models.efficientnet_b3(pretrained=pretrain)\n",
    "        elif b == 2:\n",
    "            self.model = torchvision.models.efficientnet_b2(pretrained=pretrain)\n",
    "        elif b == 1:\n",
    "            self.model = torchvision.models.efficientnet_b1(pretrained=pretrain)\n",
    "        elif b == 0:\n",
    "            self.model = torchvision.models.efficientnet_b0(pretrained=pretrain)\n",
    "            self.model.load_state_dict(torch.load('efficientnet_b0_rwightman-3dd342df.pth'))\n",
    "        new_proj = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        print('conv1 get from pretrained model.')\n",
    "        new_proj.weight = torch.nn.Parameter(torch.sum(self.model.features[0][0].weight, dim=1).unsqueeze(1))\n",
    "        new_proj.bias = self.model.features[0][0].bias\n",
    "        self.model.features[0][0] = new_proj\n",
    "        # self.model = create_feature_extractor(self.model, {'features.8': 'mout'})\n",
    "        self.model.avgpool = Identity()\n",
    "        self.model.classifier = Identity()\n",
    "        # print(self.model)\n",
    "        self.feat_dim, self.freq_dim = self.get_dim()\n",
    "        print(self.feat_dim)\n",
    "        self.attention = MeanPooling(self.feat_dim, label_dim)\n",
    "\n",
    "    def get_dim(self):\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = torch.zeros(10, 1000, 40)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "        # print(x.shape)\n",
    "        x = self.model.features(x)\n",
    "        # print(x.shape[1], x.shape[2])\n",
    "        return int(x.shape[1]), int(x.shape[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "        x = self.model.features(x)\n",
    "        out = torch.sigmoid(self.attention(x))\n",
    "        return out\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.layernorm = nn.LayerNorm(n_in)\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        print('use mean pooling')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"input: (samples_num, freq_bins, time_steps, 1)\n",
    "        \"\"\"\n",
    "        x = torch.mean(x, dim=[2, 3])\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3eb256-16dd-4bdb-bedb-81b0426a184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now train a effnet-b0 model\n",
      "conv1 get from pretrained model.\n",
      "1280\n",
      "use mean pooling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eff_net = EffNetOri()\n",
    "batch = next(iter(tr_ds))\n",
    "eff_net(batch['feats']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b6331-d278-46b0-9ca2-a3419e5ad5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_wav_scp = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/train_fra/feats.scp'\n",
    "tr_utt2label = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/train_fra/utt2label'\n",
    "cmvn_file = '/data/sls/scratch/sameerk/fhs_prepared/kaldi_data/fbanks/cmvn.ark'\n",
    "tr_ds = FHSData(tr_wav_scp, tr_utt2label, cmvn_file)\n",
    "tr_ds = torch.utils.data.DataLoader(tr_ds, batch_size=32, collate_fn=_collate_fn, drop_last=False, \n",
    "                                    num_workers=2, shuffle=True)\n",
    "\n",
    "dev_wav_scp = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/dev_fra/feats.scp'\n",
    "dev_utt2label = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/dev_fra/utt2label'\n",
    "cmvn_file = '/data/sls/scratch/sameerk/fhs_prepared/kaldi_data/fbanks/cmvn.ark'\n",
    "dev_ds = FHSData(dev_wav_scp, dev_utt2label, cmvn_file)\n",
    "dev_ds = torch.utils.data.DataLoader(dev_ds, batch_size=32, collate_fn=_collate_fn, drop_last=False, \n",
    "                                    num_workers=2, shuffle=False)\n",
    "\n",
    "\n",
    "test_wav_scp = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/test_fra/feats.scp'\n",
    "test_utt2label = '/data/sls/u/sameerk/code/kaldi/egs/librispeech/s5/data/test_fra/utt2label'\n",
    "cmvn_file = '/data/sls/scratch/sameerk/fhs_prepared/kaldi_data/fbanks/cmvn.ark'\n",
    "test_ds = FHSData(test_wav_scp, test_utt2label, cmvn_file)\n",
    "test_ds = torch.utils.data.DataLoader(test_ds, batch_size=128, collate_fn=_collate_fn, drop_last=False, \n",
    "                                      num_workers=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae9473-2e82-4cf0-b79f-0bb73f0e6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "for wd in [1e-7, 1e-5, 1e-3]:\n",
    "    for lr in [0.01, 0.005, 0.08, 0.03, 0.001]:\n",
    "        for seed in [3333, 1111, 6825, 2222, 1234, 5555, 10001, 345667]:\n",
    "            print(\"++++++ Exp seed %d wd %s lr %f ++++++++\" % (seed, str(wd), lr))\n",
    "            torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            model = EffNetOri()\n",
    "            model.cuda()\n",
    "            optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "\n",
    "            auc_best = 0.\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for batch in tqdm.tqdm(tr_ds, total=len(tr_ds)):\n",
    "                    x = batch['feats']\n",
    "                    # use lens to do average pooling properly\n",
    "                    x_lens = batch['feats_len']\n",
    "                    tgts = batch['targets']\n",
    "                    x = x.cuda()\n",
    "                    tgts = tgts.cuda()\n",
    "                    x = model(x).squeeze(1)\n",
    "                    loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    epoch_loss += float(loss)\n",
    "                    y_pred.append(x.detach().cpu().numpy())\n",
    "                    y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                print('epoch loss: %f' % (epoch_loss/len(tr_ds)))\n",
    "                print('auc: %f' % auc)\n",
    "\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm.tqdm(dev_ds, total=len(dev_ds)):\n",
    "                        x = batch['feats']\n",
    "                        # use lens to do average pooling properly\n",
    "                        x_lens = batch['feats_len']\n",
    "                        tgts = batch['targets']\n",
    "                        x = x.cuda()\n",
    "                        tgts = tgts.cuda()\n",
    "                        x = model(x).squeeze(1)\n",
    "                        loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                        epoch_loss += float(loss)\n",
    "                        y_pred.append(x.detach().cpu().numpy())\n",
    "                        y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                if auc > auc_best:\n",
    "                    print(\"saving ckpt at auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    torch.save(model.state_dict(), \"auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    auc_best = auc\n",
    "\n",
    "                print('epoch dev loss: %f' % (epoch_loss/len(dev_ds)))\n",
    "                print('auc dev: %f' % auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ee1cc-9f49-4abb-a4e5-bf463ecfe13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "    \n",
    "y_pred = []\n",
    "y_true = []\n",
    "model = EffNetOri()\n",
    "model.load_state_dict(torch.load('auc_0.911797_seed_10001_wd_1e-07_lr_0.030000.pt'))\n",
    "print(m)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(dev_ds, total=len(dev_ds)):\n",
    "        x = batch['feats']\n",
    "        # use lens to do average pooling properly\n",
    "        x_lens = batch['feats_len']\n",
    "        tgts = batch['targets']\n",
    "        x = x.cuda()\n",
    "        tgts = tgts.cuda()\n",
    "        x = model(x).squeeze(1)\n",
    "        y_pred.append(x.detach().cpu().numpy())\n",
    "        y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "print(auc)\n",
    "y_pred_class = (np.concatenate(y_pred)>0.5).astype(float)\n",
    "print(classification_report(np.concatenate(y_true), y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df766f46-574f-47a1-a31e-ff7faca036a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that in binary classification, recall of the positive class is also known as “sensitivity”; recall of the negative class is “specificity”.\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_class = (np.concatenate(y_pred)>0.5).astype(float)\n",
    "print(classification_report(np.concatenate(y_true), y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e2a16-5ef8-492e-b59b-b923d3eb856a",
   "metadata": {},
   "source": [
    "### KD with small model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4000b-a336-4bcc-bc88-2f0b5c7ec221",
   "metadata": {},
   "source": [
    "#### load big models for KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd63180-40b7-46c2-ad9a-eeacf328aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_net = EffNetOri()\n",
    "big_cnn = CNN()\n",
    "eff_net.load_state_dict(torch.load('models_effnet/auc_0.911797_seed_10001_wd_1e-07_lr_0.030000.pt'))\n",
    "big_cnn.load_state_dict(torch.load('models/auc_0.852169_seed_2222_wd_1e-07_lr_0.010000.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3a883-f1cf-4699-a858-a712d7335c1d",
   "metadata": {},
   "source": [
    "#### perform KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf68253-0c92-417b-98a0-e17ec33bcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNSmallV1(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(CNNSmallV1, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # defining batchnorm input                                                                                               \n",
    "        # self.batchnorm1 = nn.BatchNorm2d(1)\n",
    "\n",
    "        # defining Convolutional layers                                                                                           \n",
    "        conv1_C = 256\n",
    "        # conv2_C = 512\n",
    "        conv3_C = 1024\n",
    "        # conv4_C = 2048\n",
    "        # conv5_C = hparams['out_channels'][4]\n",
    "        conv_W = 18\n",
    "        pad = int(np.floor(conv_W / 2) - 1)\n",
    "        stride_W = 2\n",
    "        pool_W = 3\n",
    "        self.conv1 = nn.Conv2d(1, conv1_C, kernel_size=(40,1), stride=(1,1), padding=(0,0))\n",
    "        self.bn1 = nn.BatchNorm2d(conv1_C)\n",
    "        # self.conv2 = nn.Conv2d(conv1_C, conv2_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        # self.bn2 = nn.BatchNorm2d(conv2_C)\n",
    "        self.conv3 = nn.Conv2d(conv1_C, conv3_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn3 = nn.BatchNorm2d(conv3_C)\n",
    "        # self.conv4 = nn.Conv2d(conv3_C, conv4_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        # self.bn4 = nn.BatchNorm2d(conv4_C)\n",
    "        # self.conv5 = nn.Conv2d(conv4_C, conv5_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "\n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "        \n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "\n",
    "        # Defining output layer                                                                                                   \n",
    "        input_W = 2 ** 11\n",
    "        num_layers = 3\n",
    "        conv_C = 1024\n",
    "        \n",
    "        if num_layers == 0:\n",
    "            num_layers = 1\n",
    "        # hard coded, figure it out manually\n",
    "        map_W = 511                                                                                  \n",
    "        embedding_dim = int(conv_C)\n",
    "        self.fc1 = nn.Linear(embedding_dim, num_classes)\n",
    "        self.fc2 = nn.Sigmoid()\n",
    "        \n",
    "        # Defining global average pooling                                                                                         \n",
    "        # self.poolMean = nn.AvgPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "\n",
    "        # Defining global max pooling                                                                                             \n",
    "        self.poolMax = nn.MaxPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        targets = batch['targets']\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # x = self.batchnorm1(x)\n",
    "        x = self.bn1(F.relu(self.conv1(x)))\n",
    "        # x = self.bn2(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(F.relu(self.conv3(x)))\n",
    "        # x = self.pool(x)\n",
    "        # x = self.bn4(F.relu(self.conv4(x)))\n",
    "        x = self.poolMax(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a0f83-d86e-474f-8056-7cff473bcf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "big_cnn.eval()\n",
    "big_cnn.cuda()\n",
    "eff_net.eval()\n",
    "eff_net.cuda()\n",
    "\n",
    "for wd in [1e-7, 1e-5, 1e-3]:\n",
    "    for lr in [0.01, 0.005, 0.08, 0.03, 0.001]:\n",
    "        for seed in [3333, 1111, 6825, 2222, 1234, 5555, 10001, 345667]:\n",
    "            print(\"++++++ Exp seed %d wd %s lr %f ++++++++\" % (seed, str(wd), lr))\n",
    "            torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            model = CNNSmallV1()\n",
    "            model.cuda()\n",
    "            optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "\n",
    "            auc_best = 0.\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for batch in tqdm.tqdm(tr_ds, total=len(tr_ds)):\n",
    "                    x = batch['feats']\n",
    "                    # use lens to do average pooling properly\n",
    "                    x_lens = batch['feats_len']\n",
    "                    tgts = batch['targets']\n",
    "                    x = x.cuda()\n",
    "                    tgts = tgts.cuda()\n",
    "                    with torch.no_grad():\n",
    "                        p_y1 = eff_net(x).squeeze(1)\n",
    "                        p_y2 = big_cnn(x).squeeze(1)\n",
    "                    x = model(x).squeeze(1)\n",
    "                    loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                    loss += F.binary_cross_entropy(x, p_y1, reduction='mean')\n",
    "                    loss += F.binary_cross_entropy(x, p_y2, reduction='mean')\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    epoch_loss += float(loss)\n",
    "                    y_pred.append(x.detach().cpu().numpy())\n",
    "                    y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                print('epoch loss: %f' % (epoch_loss/len(tr_ds)))\n",
    "                print('auc: %f' % auc)\n",
    "\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm.tqdm(dev_ds, total=len(dev_ds)):\n",
    "                        x = batch['feats']\n",
    "                        # use lens to do average pooling properly\n",
    "                        x_lens = batch['feats_len']\n",
    "                        tgts = batch['targets']\n",
    "                        x = x.cuda()\n",
    "                        tgts = tgts.cuda()\n",
    "                        x = model(x).squeeze(1)\n",
    "                        loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                        epoch_loss += float(loss)\n",
    "                        y_pred.append(x.detach().cpu().numpy())\n",
    "                        y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                if auc > auc_best:\n",
    "                    print(\"saving ckpt at auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    torch.save(model.state_dict(), \"auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    auc_best = auc\n",
    "\n",
    "                print('epoch dev loss: %f' % (epoch_loss/len(dev_ds)))\n",
    "                print('auc dev: %f' % auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f669286-0f3b-4440-9e8a-b4c685ddd209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNSmallV2(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(CNNSmallV2, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # defining batchnorm input                                                                                               \n",
    "        # self.batchnorm1 = nn.BatchNorm2d(1)\n",
    "\n",
    "        # defining Convolutional layers                                                                                           \n",
    "        conv1_C = 128\n",
    "        # conv2_C = 512\n",
    "        conv3_C = 256\n",
    "        # conv4_C = 2048\n",
    "        # conv5_C = hparams['out_channels'][4]\n",
    "        conv_W = 18\n",
    "        pad = int(np.floor(conv_W / 2) - 1)\n",
    "        stride_W = 2\n",
    "        pool_W = 3\n",
    "        self.conv1 = nn.Conv2d(1, conv1_C, kernel_size=(40,1), stride=(1,1), padding=(0,0))\n",
    "        self.bn1 = nn.BatchNorm2d(conv1_C)\n",
    "        # self.conv2 = nn.Conv2d(conv1_C, conv2_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        # self.bn2 = nn.BatchNorm2d(conv2_C)\n",
    "        self.conv3 = nn.Conv2d(conv1_C, conv3_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn3 = nn.BatchNorm2d(conv3_C)\n",
    "        # self.conv4 = nn.Conv2d(conv3_C, conv4_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        # self.bn4 = nn.BatchNorm2d(conv4_C)\n",
    "        # self.conv5 = nn.Conv2d(conv4_C, conv5_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "\n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "        \n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "\n",
    "        # Defining output layer                                                                                                   \n",
    "        input_W = 2 ** 11\n",
    "        num_layers = 3\n",
    "        conv_C = 256\n",
    "        \n",
    "        if num_layers == 0:\n",
    "            num_layers = 1\n",
    "        # hard coded, figure it out manually\n",
    "        map_W = 511                                                                                  \n",
    "        embedding_dim = int(conv_C)\n",
    "        self.fc1 = nn.Linear(embedding_dim, num_classes)\n",
    "        self.fc2 = nn.Sigmoid()\n",
    "        \n",
    "        # Defining global average pooling                                                                                         \n",
    "        # self.poolMean = nn.AvgPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "\n",
    "        # Defining global max pooling                                                                                             \n",
    "        self.poolMax = nn.MaxPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        targets = batch['targets']\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # x = self.batchnorm1(x)\n",
    "        x = self.bn1(F.relu(self.conv1(x)))\n",
    "        # x = self.bn2(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(F.relu(self.conv3(x)))\n",
    "        # x = self.pool(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.bn4(F.relu(self.conv4(x)))\n",
    "        x = self.poolMax(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30645864-cb2d-4407-8386-9e48a2149584",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "big_cnn.eval()\n",
    "big_cnn.cuda()\n",
    "eff_net.eval()\n",
    "eff_net.cuda()\n",
    "\n",
    "for wd in [1e-7, 1e-5, 1e-3]:\n",
    "    for lr in [0.01, 0.005, 0.08, 0.03, 0.001]:\n",
    "        for seed in [3333, 1111, 6825, 2222, 1234, 5555, 10001, 345667]:\n",
    "            print(\"++++++ Exp seed %d wd %s lr %f ++++++++\" % (seed, str(wd), lr))\n",
    "            torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            model = CNNSmallV2()\n",
    "            model.cuda()\n",
    "            optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "\n",
    "            auc_best = 0.\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for batch in tqdm.tqdm(tr_ds, total=len(tr_ds)):\n",
    "                    x = batch['feats']\n",
    "                    # use lens to do average pooling properly\n",
    "                    x_lens = batch['feats_len']\n",
    "                    tgts = batch['targets']\n",
    "                    x = x.cuda()\n",
    "                    tgts = tgts.cuda()\n",
    "                    with torch.no_grad():\n",
    "                        p_y1 = eff_net(x).squeeze(1)\n",
    "                        p_y2 = big_cnn(x).squeeze(1)\n",
    "                    x = model(x).squeeze(1)\n",
    "                    loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                    loss += F.binary_cross_entropy(x, p_y1, reduction='mean')\n",
    "                    loss += F.binary_cross_entropy(x, p_y2, reduction='mean')\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    epoch_loss += float(loss)\n",
    "                    y_pred.append(x.detach().cpu().numpy())\n",
    "                    y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                print('epoch loss: %f' % (epoch_loss/len(tr_ds)))\n",
    "                print('auc: %f' % auc)\n",
    "\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm.tqdm(dev_ds, total=len(dev_ds)):\n",
    "                        x = batch['feats']\n",
    "                        # use lens to do average pooling properly\n",
    "                        x_lens = batch['feats_len']\n",
    "                        tgts = batch['targets']\n",
    "                        x = x.cuda()\n",
    "                        tgts = tgts.cuda()\n",
    "                        x = model(x).squeeze(1)\n",
    "                        loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                        epoch_loss += float(loss)\n",
    "                        y_pred.append(x.detach().cpu().numpy())\n",
    "                        y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                if auc > auc_best:\n",
    "                    print(\"saving ckpt at auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    torch.save(model.state_dict(), \"auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    auc_best = auc\n",
    "\n",
    "                print('epoch dev loss: %f' % (epoch_loss/len(dev_ds)))\n",
    "                print('auc dev: %f' % auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5e1242f-9471-48f6-9c4f-b49d762c3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNSmallV3(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(CNNSmallV3, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # defining batchnorm input                                                                                               \n",
    "        # self.batchnorm1 = nn.BatchNorm2d(1)\n",
    "\n",
    "        # defining Convolutional layers                                                                                           \n",
    "        conv1_C = 64\n",
    "        # conv2_C = 512\n",
    "        conv3_C = 128\n",
    "        # conv4_C = 2048\n",
    "        # conv5_C = hparams['out_channels'][4]\n",
    "        conv_W = 18\n",
    "        pad = int(np.floor(conv_W / 2) - 1)\n",
    "        stride_W = 2\n",
    "        pool_W = 3\n",
    "        self.conv1 = nn.Conv2d(1, conv1_C, kernel_size=(40,1), stride=(1,1), padding=(0,0))\n",
    "        self.bn1 = nn.BatchNorm2d(conv1_C)\n",
    "        # self.conv2 = nn.Conv2d(conv1_C, conv2_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        # self.bn2 = nn.BatchNorm2d(conv2_C)\n",
    "        self.conv3 = nn.Conv2d(conv1_C, conv3_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        self.bn3 = nn.BatchNorm2d(conv3_C)\n",
    "        # self.conv4 = nn.Conv2d(conv3_C, conv4_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "        # self.bn4 = nn.BatchNorm2d(conv4_C)\n",
    "        # self.conv5 = nn.Conv2d(conv4_C, conv5_C, kernel_size=(1,conv_W), stride=(1,1), padding=(0,pad))\n",
    "\n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "        \n",
    "        # Defining pooling                                                                                                        \n",
    "        pad = int(np.floor(pool_W / 2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1,pool_W), stride=(1,stride_W),padding=(0,pad))\n",
    "\n",
    "        # Defining output layer                                                                                                   \n",
    "        input_W = 2 ** 11\n",
    "        num_layers = 3\n",
    "        conv_C = 128\n",
    "        \n",
    "        if num_layers == 0:\n",
    "            num_layers = 1\n",
    "        # hard coded, figure it out manually\n",
    "        map_W = 511                                                                                  \n",
    "        embedding_dim = int(conv_C)\n",
    "        self.fc1 = nn.Linear(embedding_dim, num_classes)\n",
    "        self.fc2 = nn.Sigmoid()\n",
    "        \n",
    "        # Defining global average pooling                                                                                         \n",
    "        # self.poolMean = nn.AvgPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "\n",
    "        # Defining global max pooling                                                                                             \n",
    "        self.poolMax = nn.MaxPool2d(kernel_size=(1,map_W), stride=(1,map_W), padding=(0,0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        targets = batch['targets']\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # x = self.batchnorm1(x)\n",
    "        x = self.bn1(F.relu(self.conv1(x)))\n",
    "        # x = self.bn2(F.relu(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(F.relu(self.conv3(x)))\n",
    "        # x = self.pool(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.bn4(F.relu(self.conv4(x)))\n",
    "        x = self.poolMax(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f395145-d54d-4bb5-8349-e1dd8d3e2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "big_cnn.eval()\n",
    "big_cnn.cuda()\n",
    "eff_net.eval()\n",
    "eff_net.cuda()\n",
    "\n",
    "for wd in [1e-7, 1e-5, 1e-3]:\n",
    "    for lr in [0.01, 0.005, 0.08, 0.03, 0.001]:\n",
    "        for seed in [3333, 1111, 6825, 2222, 1234, 5555, 10001, 345667]:\n",
    "            print(\"++++++ Exp seed %d wd %s lr %f ++++++++\" % (seed, str(wd), lr))\n",
    "            torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            model = CNNSmallV3()\n",
    "            model.cuda()\n",
    "            optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "\n",
    "            auc_best = 0.\n",
    "            for epoch in range(5):\n",
    "                model.train()\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for batch in tqdm.tqdm(tr_ds, total=len(tr_ds)):\n",
    "                    x = batch['feats']\n",
    "                    # use lens to do average pooling properly\n",
    "                    x_lens = batch['feats_len']\n",
    "                    tgts = batch['targets']\n",
    "                    x = x.cuda()\n",
    "                    tgts = tgts.cuda()\n",
    "                    with torch.no_grad():\n",
    "                        p_y1 = eff_net(x).squeeze(1)\n",
    "                        p_y2 = big_cnn(x).squeeze(1)\n",
    "                    x = model(x).squeeze(1)\n",
    "                    loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                    loss += F.binary_cross_entropy(x, p_y1, reduction='mean')\n",
    "                    loss += F.binary_cross_entropy(x, p_y2, reduction='mean')\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    epoch_loss += float(loss)\n",
    "                    y_pred.append(x.detach().cpu().numpy())\n",
    "                    y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                print('epoch loss: %f' % (epoch_loss/len(tr_ds)))\n",
    "                print('auc: %f' % auc)\n",
    "\n",
    "                epoch_loss = 0.\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm.tqdm(dev_ds, total=len(dev_ds)):\n",
    "                        x = batch['feats']\n",
    "                        # use lens to do average pooling properly\n",
    "                        x_lens = batch['feats_len']\n",
    "                        tgts = batch['targets']\n",
    "                        x = x.cuda()\n",
    "                        tgts = tgts.cuda()\n",
    "                        x = model(x).squeeze(1)\n",
    "                        loss = F.binary_cross_entropy(x, tgts.float(), reduction='mean')\n",
    "                        epoch_loss += float(loss)\n",
    "                        y_pred.append(x.detach().cpu().numpy())\n",
    "                        y_true.append(tgts.cpu().numpy())\n",
    "\n",
    "                auc = roc_auc_score(np.concatenate(y_true), np.concatenate(y_pred))\n",
    "\n",
    "                if auc > auc_best:\n",
    "                    print(\"saving ckpt at auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    torch.save(model.state_dict(), \"auc_%f_seed_%d_wd_%s_lr_%f.pt\" % (auc, seed, str(wd), lr))\n",
    "                    auc_best = auc\n",
    "\n",
    "                print('epoch dev loss: %f' % (epoch_loss/len(dev_ds)))\n",
    "                print('auc dev: %f' % auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bcd4c-53f7-4ad2-ae67-da23cc22d027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair_nov25",
   "language": "python",
   "name": "fair_nov25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
